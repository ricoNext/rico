# 02 AI 基础调用实现：从原理到代码实践 ​

> 在上篇文章中，我们介绍了 AI 技术从基础调用到自主智能体的应用层演进，后续会基于这个脉络着眼各阶段的关键技术实现，和技术层面的落地。

本文介绍基础 LLM 调用的实现，包括模型选择、Prompt 设计、参数配置等。 同时，我们会基于实际代码案例，演示如何在不同的开发环境中调用基础 LLM。

## 模型基础调用的实现

首先看模型基础调用的应用架构图：

![](https://neptune-ipc.oss-cn-shenzhen.aliyuncs.com/img/20250825151704119.png)

我们需要一个客户端向服务层传输请求，服务层会调用模型层的 API，LLM 会返回结果，服务层再将结果返回给客户端。

服务端这里选择使用 node 实现，为了方便客户端搭建，这里使用 NextJs 举例。 对接的模型使用 Deepseek。

由于市面上大部分大模型的 API 调用都兼容 OpenAI API 的格式，这里选用 OpenAI 的 node SDK 访问大模型服务。

openai 实例初始化需要传入 apiKey 和 baseURL，apiKey 和 baseURL 从使用的大模型平台获取。

为了安全起见，不建议将 apiKey 直接写在代码中，而是从环境变量中获取。 在项目中，我们可以在 `env.local` 文件中定义环境变量，以 Deepseek 的 api 对接为例，在 env 中定义 Deepseek 的 apikey 和 baseUrl

```bash
DEEPSEEK_API_KEY=xxxxxx
DEEPSEEK_API_BASE_URL=https://api.deepseek.com
```

安装 openai 的 node SDK：

```bash
pnpm add openai
```

初始化 openai 实例：

```typescript
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: process.env.DEEPSEEK_API_KEY,
  baseURL: process.env.DEEPSEEK_API_BASE_URL,
});
```

创建对话：

```typescript
export async function POST(request: Request) {
  const completion = await openai.chat.completions.create({
    model: "deepseek-chat",
    messages: [{ role: "system", content: "You are a helpful assistant." }],
  });
  return new Response(completion.choices[0].message.content);
}
```

这里 create 方法接收两个参数：model 和 messages。

- model 是模型名称，这里选择 deepseek-chat。 也可以选择其他的模型 [模型列表](https://api-docs.deepseek.com/zh-cn/quick_start/pricing)

- messages 是对话历史，也就是常听到的 Prompt。 关于 Prompt 下面会做详细的解析， 这里需要知道对于 Prompt 需要传入 role 和 content 两个参数。

客户端调用对话：

当点击按钮时，调用服务端的 API，服务端会返回模型的结果。

```typescript
export default function Home() {
  const [data, setData] = useState("");

  const handleClick = async () => {
    const res = await fetch("/api/chat", {
      method: "POST",
    });
    const data = await res.text();
    setData(data);
  };
  return (
    <div>
      <Button onClick={handleClick}>点击我</Button>
      <div>{data}</div>
    </div>
  );
}
```

### messages 参数讲解

messages 参数是一个数组，每个元素都是一个对象，对象包含 role 和 content 两个参数， 每个对象也就是一个 Prompt 指令。

Prompt 就像是传统 http 协议中传递给服务端的参数，http server 会根据参数返回不同的结果，LLM 也会根据传入的 prompt 内容返回不同的结果。 但不同与 http server 的是，http server 会定义传入参数，对于非法参数 http server 并不会接受处理，但 LLM 什么参数都会接收和处理，所以为了提升 LLM 处理的准确性，需要向 LLM 传递更加明确的意图指令参数。

所以通用大模型对于 Prompt 参数首先要求传入 role 和 该 role 对应的 content， 也就是和大模型进行对话时， 需要告知大模型的角色定位和角色定位的描述和要求。

同时为了提升 role 的灵活性， 大模型也支持传入多组 role 和 content：

- system 角色： 系统角色，设定模型的行为边界、身份定位和全局规则。
- user 角色： 用户角色，传递用户的具体需求或问题，触发模型生成回复。
- assistant 角色： 助手角色，承载模型的响应内容，维持对话连贯性。

一般我们创建一个新的对话的时候， 应该先传递 system 角色， 系统角色设定模型的行为边界、身份定位和全局规则。后续的问答角色是 user 角色， 而对应的 LLM 的角色是 assistant。

关于 Prompt 的设计是一个分场景的工程化问题， 后续的文章中会继续探究。下面回到 LLM 的调用上，急需补齐基础通信的能力。

### 多轮对话实现

在上面的代码中，我们只传递了一个固定的 Prompt 指令， 大模型会根据这个指令返回结果。 但在实际场景中， 我们需要输入并持续传递多轮对话指令，我们来看一下这部分的实现。

服务端接收客户端 prompt 指令：

```typescript
export async function POST(request: Request) {
  const { prompt } = await request.json();
  const completion = await openai.chat.completions.create({
    model: "deepseek-chat",
    messages: [{ role: "system", content: "You are a helpful assistant." }],
  });
  return new Response(completion.choices[0].message.content);
}
```
